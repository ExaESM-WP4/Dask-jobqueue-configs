{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to customize your Dask jobqueue cluster\n",
    "\n",
    "covers the following aspects, i.e. how to\n",
    "* choose a NEC Linux cluster queue\n",
    "* adjust Dask jobqueue worker resources\n",
    "* scale Dask clusters adaptively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a NEC Linux cluster queue\n",
    "\n",
    "The [NEC Linux cluster system](https://www.rz.uni-kiel.de/de/angebote/hiperf/nec-linux-cluster) has a theoretical number of 198 batch nodes with 24-32 CPUs and 128-384 GB memory available per execution host. Dask jobqueue workers are intended to have rather short lifetimes and would therefore survive with the walltime limits set for the `clexpress` batch queue. However, the very limited node/execution host number might lead to very unpredictable Dask worker job execution starting times (note the [spiky workload tendency](https://nbviewer.jupyter.org/github/ExaESM-WP4/nesh-monitoring/blob/v2020.01.24.1/analysis.ipynb)), which is not good for a default configuration. Therefore, the batch class `clmedium` has been chosen as default. A [relatively stable number of 200-400 idle CPUs](https://nbviewer.jupyter.org/github/ExaESM-WP4/nesh-monitoring/blob/v2020.01.24.1/analysis.ipynb) (at least during the considered log period!) is observed and (not too big!) Dask jobqueue clusters should always immediately start/connect. However, as available resources generally fluctuate with present user behaviour (which might change!), below you find an example on how to get an overview on currently available CPU resources and how to set up your Dask jobqueue cluster in a different than the default batch class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get details on available batch class resources\n",
    "\n",
    "Ther [recommended way](https://www.rz.uni-kiel.de/de/angebote/hiperf/nec-linux-cluster) of getting an overview on available batch queue resources is via the `qcl` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch class  Walltime[h]  Max.cores/node  Max.RAM[gb]  Total[*]  Used[*]  Avail[*]  Run.jobs/user\n",
      " -----------  -----------  --------------  -----------  --------  -------  --------  -------------\n",
      " clexpress              2              32          192         2        2         0              -\n",
      " clmedium              48              32          192       116      116         0             20\n",
      " cllong               100              32          192        49       49         0             10\n",
      " clbigmem             200              32          384         8        8         0              2\n",
      " clfo2                200              24          128        17       17         0              8\n",
      " feque                  1              32           64         1        1         0              -\n",
      " -----------  -----------  --------------  -----------  --------  -------  --------  -------------\n",
      " Sum                                                         193      193         0\n",
      " -----------  -----------  --------------  -----------  --------  -------  --------  -------------\n",
      " [*] = number of neshcl### nodes\n"
     ]
    }
   ],
   "source": [
    "!qcl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the NEC Linux cluster is a shared host/node system and `qcl` doesn't provide useful details on actually allocated resources. The following `qstat` command lists free resources of the currently active nodes/execution hosts grouped alphabetically by batch queue class, filtered for nodes that have at least one unoccupied CPU. It gives an overview on theoretically available resources and might help with choosing a batch queue for your Dask jobqueue cluster. Please note, that each batch class has its own running job number limit that constrains the total number of Dask workers and hence your Dask cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free_CPUs  Free_Mem1 Used_CPUs  Used_Mem1 ExecutionHost   QueueName      \n",
      "--------- ---------- --------- ---------- --------------- ---------------\n",
      "       16    334.586        16     48.125 neshcl219       clbigmem       \n",
      "       30    363.465         2     19.246 neshcl224       clbigmem       \n",
      "       30    373.582         2      9.129 neshcl225       clbigmem       \n",
      "       31    376.113         1      6.598 neshcl220       clbigmem       \n",
      "       31    377.371         1      5.340 neshcl221       clbigmem       \n",
      "        3    110.418        21     17.477 neshcl216       clfo2          \n",
      "        4    101.266        20     26.629 neshcl209       clfo2          \n",
      "        4    101.629        20     26.266 neshcl211       clfo2          \n",
      "        4    101.742        20     26.152 neshcl201       clfo2          \n",
      "        4    102.043        20     25.852 neshcl207       clfo2          \n",
      "        4    102.625        20     25.270 neshcl217       clfo2          \n",
      "        4    105.160        20     22.734 neshcl215       clfo2          \n",
      "        4    105.688        20     22.207 neshcl208       clfo2          \n",
      "        4    105.863        20     22.031 neshcl203       clfo2          \n",
      "        4    109.965        20     17.930 neshcl200       clfo2          \n",
      "        4    110.223        20     17.672 neshcl213       clfo2          \n",
      "       10    165.035        22     25.676 neshcl364       cllong         \n",
      "       10    165.879        22     24.832 neshcl348       cllong         \n",
      "       10    169.770        22     20.941 neshcl352       cllong         \n",
      "       12    161.445        20     29.266 neshcl367       cllong         \n",
      "       12    165.141        20     25.570 neshcl378       cllong         \n",
      "       12    165.211        20     25.500 neshcl392       cllong         \n",
      "       12    168.848        20     21.863 neshcl371       cllong         \n",
      "       13    156.832        19     33.879 neshcl380       cllong         \n",
      "       13    173.480        19     17.230 neshcl360       cllong         \n",
      "       14    142.973        18     47.738 neshcl376       cllong         \n",
      "       14    177.219        18     13.492 neshcl386       cllong         \n",
      "       25    143.242         7     47.469 neshcl390       cllong         \n",
      "       27    165.004         5     25.707 neshcl368       cllong         \n",
      "       27    186.824         5      3.887 neshcl356       cllong         \n",
      "       28    153.117         4     37.594 neshcl354       cllong         \n",
      "       28    181.199         4      9.512 neshcl387       cllong         \n",
      "       30    154.660         2     36.051 neshcl363       cllong         \n",
      "       30    170.984         2     19.727 neshcl396       cllong         \n",
      "       30    173.418         2     17.293 neshcl350       cllong         \n",
      "       31    156.836         1     33.875 neshcl366       cllong         \n",
      "        4    160.504        28     30.207 neshcl351       cllong         \n",
      "        6    169.426        26     21.285 neshcl395       cllong         \n",
      "        6    178.137        26     12.574 neshcl374       cllong         \n",
      "        7    182.180        25      8.531 neshcl393       cllong         \n",
      "       10    170.605        22     20.105 neshcl252       clmedium       \n",
      "       12    162.438        20     28.273 neshcl300       clmedium       \n",
      "       12    163.422        20     27.289 neshcl267       clmedium       \n",
      "       12    164.902        20     25.809 neshcl268       clmedium       \n",
      "       12    165.273        20     25.438 neshcl260       clmedium       \n",
      "       12    165.453        20     25.258 neshcl303       clmedium       \n",
      "       12    167.855        20     22.855 neshcl280       clmedium       \n",
      "       12    168.238        20     22.473 neshcl293       clmedium       \n",
      "       12    168.281        20     22.430 neshcl257       clmedium       \n",
      "       12    170.238        20     20.473 neshcl230       clmedium       \n",
      "       12    170.602        20     20.109 neshcl253       clmedium       \n",
      "       12    170.672        20     20.039 neshcl332       clmedium       \n",
      "       12    170.727        20     19.984 neshcl305       clmedium       \n",
      "       12    171.816        20     18.895 neshcl277       clmedium       \n",
      "       14    169.398        18     21.312 neshcl310       clmedium       \n",
      "       22    169.422        10     21.289 neshcl279       clmedium       \n",
      "       32    173.406         0     17.305 neshcl231       clmedium       \n",
      "       32    188.000         0      2.711 neshcl313       clmedium       \n",
      "        4      4.082        28    186.629 neshcl341       clmedium       \n",
      "        5    122.023        27     68.688 neshcl301       clmedium       \n",
      "        6      3.613        26    187.098 neshcl287       clmedium       \n",
      "        6      4.855        26    185.855 neshcl299       clmedium       \n",
      "        6     98.504        26     92.207 neshcl284       clmedium       \n",
      "        7    100.797        25     89.914 neshcl292       clmedium       \n",
      "        7      3.625        25    187.086 neshcl250       clmedium       \n",
      "        7     75.750        25    114.961 neshcl294       clmedium       \n",
      "        8    139.859        24     50.852 neshcl334       clmedium       \n",
      "        8      3.680        24    187.031 neshcl262       clmedium       \n",
      "       32    184.520         0      6.191 neshcl343       cltestque      \n",
      "       32    179.602         0     11.109 neshcl342       cltestque,clinteractive\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qstat -E -F fcpu,fmem1,ucpu,umem1,ehost,quenm | head -n2 # provide header\n",
    "#qstat -E -F fcpu,fmem1,ucpu,umem1,ehost,quenm | awk '{ if ($1>0 && substr($5,1,6)==\"neshcl\") { print } }' | sort -k6 # raw output\n",
    "qstat -E -F fcpu,fmem1,ucpu,umem1,ehost,quenm | awk '{ if ($1>0 && substr($5,1,6)==\"neshcl\") { printf \"%9d%11.3f%10d%11.3f %-15s %-15s\\n\",$1,$2/2**8,$3,$4/2**8,$5,$6 } }' | sort -k6 # memory units in GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dask jobqueue cluster in a different batch class\n",
    "Manually choosing a batch queue for your Dask jobqueue cluster is as simple as using the default configuration and calling the `PBSCluster` method with the `queue` keyword argument. This simply alters the PBS header specifying the batch class in the Dask worker job script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nesh-jobqueue-config': {'cores': 4,\n",
       "  'memory': '24GB',\n",
       "  'processes': 1,\n",
       "  'queue': 'clmedium',\n",
       "  'resource_spec': 'elapstim_req=00:45:00,cpunum_job=4,memsz_job=24gb',\n",
       "  'interface': 'ib0',\n",
       "  'local-directory': '/scratch',\n",
       "  'walltime': None,\n",
       "  'job-extra': ['-o dask_jobqueue_logs/dask-worker.o%s',\n",
       "   '-e dask_jobqueue_logs/dask-worker.e%s'],\n",
       "  'project': None,\n",
       "  'name': 'dask-worker',\n",
       "  'death-timeout': 60,\n",
       "  'extra': [],\n",
       "  'env-extra': [],\n",
       "  'log-directory': None,\n",
       "  'shebang': '#!/bin/bash'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os; os.environ['DASK_CONFIG']='.'\n",
    "import dask.config; dask.config.get('jobqueue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_jobqueue\n",
    "custom_queue_cluster = dask_jobqueue.PBSCluster(config_name='nesh-jobqueue-config',queue='clbigmem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q clbigmem\n",
      "#PBS -l elapstim_req=00:45:00,cpunum_job=4,memsz_job=24gb\n",
      "#PBS -o dask_jobqueue_logs/dask-worker.o%s\n",
      "#PBS -e dask_jobqueue_logs/dask-worker.e%s\n",
      "JOB_ID=${PBS_JOBID%%.*}\n",
      "\n",
      "/sfs/fs6/home-geomar/smomw260/miniconda3/envs/dask-minimal-20191218/bin/python -m distributed.cli.dask_worker tcp://192.168.31.10:32866 --nthreads 4 --memory-limit 24.00GB --name name --nanny --death-timeout 60 --local-directory /scratch --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(custom_queue_cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Dask jobqueue worker resources\n",
    "Getting details on currently available batch class resources might also help in guiding your choice on single Dask worker resources. The default values are chosen to allocate one eighth (a subjective choice!) of the average `clmedium` batch class host resources. However, depending on your dataset characteristics, the calculation/operation properties and the currently occupied Linux cluster resources, other values might be more useful.\n",
    "\n",
    "Please note, that available [resources at the NEC Linux cluster system are mostly CPU limited, with plenty of memory available](https://nbviewer.jupyter.org/github/ExaESM-WP4/nesh-monitoring/blob/v2020.01.24.1/analysis.ipynb). Below you therefore find an example of how to decrease single Dask worker resources in terms of CPU allocation. Such Dask workers might occasionally better squeeze into the available NEC Linux cluster resources. In terms of total available Dask cluster resources, you can compensate the smaller single Dask worker size by increasing the total number of Dask jobqueue workers, i.e. using a larger value for the `jobs` keyword in `cluster.scale(jobs=4)`.\n",
    "\n",
    "However, this approach might both increase or decrease total execution time of your calculation, depending on the task types associated with your operation and the actual load on the node/execution host. If you play around with jobqueue worker resources (and total worker numbers) never forget that your total Dask cluster size is not only limited by the available resources and the batch class running job number limit, but also by the resources demanded by other already queued jobs (which influence the starting times of your Dask jobqueue workers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nesh-jobqueue-config': {'cores': 4,\n",
       "  'memory': '24GB',\n",
       "  'processes': 1,\n",
       "  'queue': 'clmedium',\n",
       "  'resource_spec': 'elapstim_req=00:45:00,cpunum_job=4,memsz_job=24gb',\n",
       "  'interface': 'ib0',\n",
       "  'local-directory': '/scratch',\n",
       "  'walltime': None,\n",
       "  'job-extra': ['-o dask_jobqueue_logs/dask-worker.o%s',\n",
       "   '-e dask_jobqueue_logs/dask-worker.e%s'],\n",
       "  'project': None,\n",
       "  'name': 'dask-worker',\n",
       "  'death-timeout': 60,\n",
       "  'extra': [],\n",
       "  'env-extra': [],\n",
       "  'log-directory': None,\n",
       "  'shebang': '#!/bin/bash'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os; os.environ['DASK_CONFIG']='.'\n",
    "import dask.config; dask.config.get('jobqueue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_jobqueue\n",
    "custom_worker_cluster = dask_jobqueue.PBSCluster(config_name='nesh-jobqueue-config',\n",
    "                                                # Define smaller Linux cluster resources per Dask worker.\n",
    "                                                resource_spec='elapstim_req=00:45:00,cpunum_job=2,memsz_job=24gb',\n",
    "                                                # Adjust Dask worker resource limits.\n",
    "                                                cores=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q clmedium\n",
      "#PBS -l elapstim_req=00:45:00,cpunum_job=2,memsz_job=12gb\n",
      "#PBS -o dask_jobqueue_logs/dask-worker.o%s\n",
      "#PBS -e dask_jobqueue_logs/dask-worker.e%s\n",
      "JOB_ID=${PBS_JOBID%%.*}\n",
      "\n",
      "/sfs/fs6/home-geomar/smomw260/miniconda3/envs/dask-minimal-20191218/bin/python -m distributed.cli.dask_worker tcp://192.168.31.10:38568 --nthreads 2 --memory-limit 12.00GB --name name --nanny --death-timeout 60 --local-directory /scratch --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(custom_worker_cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Dask clusters adaptively\n",
    "You can operate a Dask jobqueue cluster using fixed scaling with the `cluster.scale(jobs=2)` method, or by using adaptive scaling with the `cluster.adapt(minimum=2, maximum=10)` method. For more examples on advanced scaling methods see for example [this Dask jobqueue workshop materials notebook](https://nbviewer.jupyter.org/github/willirath/dask_jobqueue_workshop_materials/blob/v1.1.0/notebooks/03_tuning_adaptive_clusters.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dask-minimal-20191218]",
   "language": "python",
   "name": "conda-env-dask-minimal-20191218-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
